{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging and Combining data  \n",
    "\n",
    "Sometimes we need to combine data. In many instances, adding additional data is a way to enrich both the questions that can be asked of your original data set and the answers that can be provided. This notebook looks at ways to  create one dataframe from two or more dataframes.  That's colloquially known as a **merge** or a **join**. There are lots of ways to do this.  We do a couple but supply references to more at the end.  \n",
    "\n",
    "Along the way we take an extended detour to review methods for **downloading** and **unzipping** compressed files.  The tools we use here have a broad range of other applications, including web scraping.  \n",
    "\n",
    "Outline:  \n",
    "\n",
    "* [MovieLens data](#movielens).  A collection of movies and individual ratings.  \n",
    "* [Automate file download](#requests).  Use the requests package to get a zipped file, then other tools to unzip it and read in the contents.  \n",
    "* [Merge movie names and ratings](#merge-movies).  Merge information from two dataframes with Pandas' `merge` function.  \n",
    "\n",
    "**Note: requires internet access to run.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd             # data package\n",
    "import matplotlib.pyplot as plt # graphics \n",
    "\n",
    "# these are new \n",
    "import requests, io             # internet and input tools  \n",
    "import zipfile as zf            # zip file tools \n",
    "import shutil                   # file management tools \n",
    "import os                       # operating system tools (check files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Primer on Merging\n",
    "\n",
    "First, lets look at some simple examples and see how merging works. Then we will explore this more with the Movie data. \n",
    "\n",
    "There are two key commands here. First here is `pd.append`. This operation is useful when we just want to stack, say, two DataFrames on top of each other. For example, in the basketball dataset, we could layer on the 2017 and 2018 data. \n",
    "\n",
    "The next important command here is ``pd.merge()`` which is a pandas method applied to two data frames. What you need to provide pandas are the two data frames, the type of merge, and the ``key`` to merge on. Below, I'm going to look at a couple of common examples that I come across, one-to-one merges and many to one merges (in the documentation there is a discussion of many to many merges). Lets see this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append\n",
    "\n",
    "This operation is fairly straigt foward, lets just stack on ontop of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_rich_data = {\"cntry\": [\"USA\", \"JPN\"],\n",
    "               \"gdp\": [100, 110]}\n",
    "\n",
    "gdp_poor_data = {\"cntry\": [\"TZA\", \"SAR\"],\n",
    "               \"gdp\": [25, 50]}\n",
    "\n",
    "df_one = pd.DataFrame(gdp_rich_data)\n",
    "df_two = pd.DataFrame(gdp_poor_data)\n",
    "\n",
    "df_one.append(df_two, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_rich_data = {\"cntry\": [\"USA\", \"JPN\"],\n",
    "               \"gdp\": [100, 110],\n",
    "                \"cpi\": [2, 3]}\n",
    "\n",
    "gdp_poor_data = {\"cntry\": [\"TZA\", \"SAR\"],\n",
    "               \"gdp_f\": [25, 50]}\n",
    "\n",
    "df_one = pd.DataFrame(gdp_rich_data)\n",
    "df_two = pd.DataFrame(gdp_poor_data)\n",
    "\n",
    "df_one.append(df_two, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "\n",
    "This is the key operation to combine two datasets on a column (as opposed to pd.append adds rows). Lets see how this works in a simple **one-to-one** merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data = {\"cntry\": [\"USA\", \"JPN\"],\n",
    "               \"gdp\": [100, 110]}\n",
    "\n",
    "cpi_data = {\"cntry\": [\"USA\", \"JPN\"],\n",
    "               \"cpi\": [10, 15]}\n",
    "\n",
    "df_one = pd.DataFrame(gdp_data)\n",
    "df_two = pd.DataFrame(cpi_data)\n",
    "\n",
    "# Here is a classic one-to-one merge\n",
    "combo = pd.merge(df_one, df_two,# left df, right df\n",
    "                 on='cntry',       # link with cntry\n",
    "                 how='left',        # add to left...does this matter here?\n",
    "                 indicator=True)  # Tells us what happend\n",
    "\n",
    "combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have two data frames, one with gdp data and one with cpi data for two countries in **both** dataframes. We want to create one dataframe out of the two initial ones. So then ``pd.merge`` takes the left data frame, the right dataframe, and then we provide some options. \n",
    "- The most important one is ``on`` this provides the key to match up the two dataframes. In this case this is the country.\n",
    "- The second important on is ``how`` this tells us the method to which the merge is undertaken. There are four options, ``left`` (which just takes the keys from the left), ``right`` (which just takes the keys on the right), ``inner`` (which just takes the intersection of the keys), and ``outer`` (which takes the union). Note here it does not matter because there is an exact one-to-one correspondence of the keys.\n",
    "- Finally, I like to use the indicator option that will generate a new column with a description of what happened. In this case, the key was found on both.\n",
    "\n",
    "Below is another example. It is still a one-to-one merge, but now there on some keys on one dataframe that are not in another..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is still a one-to-one merge, but a bit more complicated...\n",
    "# Since the two dataframes have different stuff missing...\n",
    "\n",
    "gdp_data = {\"cntry\": [\"USA\", \"JPN\", \"BRZ\"],\n",
    "               \"gdp\": [100, 110, 50]}\n",
    "cpi_data = {\"cntry\": [\"USA\", \"JPN\", \"CHN\"],\n",
    "               \"cpi\": [10, 15, 8]}\n",
    "           #\"gdp\": [100, 110, 75]}\n",
    "\n",
    "df_one = pd.DataFrame(gdp_data)\n",
    "df_two = pd.DataFrame(cpi_data)\n",
    "\n",
    "# Here is a classic one-to-one merge\n",
    "combo = pd.merge(df_one, df_two,   # left df, right df\n",
    "                 how='left',      # Try the different options, inner, outer, left, right...what happens.\n",
    "                 on='cntry',       # link with cntry\n",
    "                 indicator=True)  # Tells us what happend\n",
    "\n",
    "combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Try different ``how`` operations...what happens?** \n",
    "\n",
    "---\n",
    "\n",
    "Now we can also merge on combinations of values. In the example below, it would be country by time merge..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data = {\"cntry\": [\"USA\", \"JPN\",\"USA\", \"JPN\"],\n",
    "               \"gdp\": [100, 110, 95, 105],\n",
    "           \"year\": [2017, 2017 , 2016, 2016] }\n",
    "\n",
    "cpi_data = {\"cntry\": [\"USA\", \"JPN\",\"USA\", \"JPN\"],\n",
    "               \"cpi\": [10, 15, 9, 14],\n",
    "            \"year\": [2017, 2017 , 2016, 2016] }\n",
    "\n",
    "df_one = pd.DataFrame(gdp_data)\n",
    "df_two = pd.DataFrame(cpi_data)\n",
    "\n",
    "# Here is a classic one-to-one merge\n",
    "combo = pd.merge(df_one, df_two,# left df, right df\n",
    "                 on=['cntry', \"year\"],       # link with cntry\n",
    "                 how='left',        # add to left...does this matter here?\n",
    "                 indicator=True)  # Tells us what happend\n",
    "\n",
    "combo.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you set the `on` option to be just `cntry`. What would happen. Do you see what is going on here?\n",
    "\n",
    "---\n",
    "\n",
    "Now below is an example of a **many-to-one merge**. That is we will assign many values to one depending upon the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_data = {\"cntry\": [\"USA\", \"JPN\", \"BRZ\"],\n",
    "               \"gdp\": [100, 110, 50]}\n",
    "\n",
    "state_pop_data = {\"cntry\": [\"USA\", \"USA\", \"JPN\", \"JPN\"],\n",
    "                  \"state\": [\"NY\", \"AK\", \"Kyoto\", \"Tokyo\"],\n",
    "                  \"pop\": [10, 5, 8, 4]}\n",
    "\n",
    "df_one = pd.DataFrame(gdp_data)\n",
    "df_two = pd.DataFrame(state_pop_data)\n",
    "\n",
    "# Here is a classic one-to-one merge\n",
    "combo = pd.merge(df_one, df_two,   # left df, right df\n",
    "                 how='inner',      # Try the different options, inner, outer, left, right...what happens.\n",
    "                 on='cntry',       # link with cntry\n",
    "                 indicator=True)  # Tells us what happend\n",
    "\n",
    "combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what is going on here. For each state (or prefecture in Japan)....(the many part) the countries GDP is assigned to the one value associated with the key, in this case GDP. Again, **Try different ``how`` operations ...what happens?** \n",
    "\n",
    "Lets see this at work for a lager, more interesting dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MovieLens data](https://grouplens.org/datasets/movielens/) \n",
    "\n",
    "The data comes as a zip file that contains several csv's.  We get the details from the README inside.  (It's written in Markdown, so it's easier to read if we use a browser to format it.  Or we could cut and paste into a Markdown cell in an IPython notebook.)  \n",
    "\n",
    "The file descriptions are:  \n",
    "\n",
    "* `ratings.csv`:  each line is an individual film rating with the rater and movie id's and the rating.  Order:  `userId, movieId, rating, timestamp`. \n",
    "* `tags.csv`:  each line is a tag on a specific film.  Order:  `userId, movieId, tag, timestamp`. \n",
    "* `movies.csv`:  each line is a movie name, its id, and its genre.  Order:  `movieId, title, genres`.  Multiple genres are separated by \"pipes\" `|`.   \n",
    "* `links.csv`:  each line contains the movie id and corresponding id's at [IMBd](http://www.imdb.com/) and [TMDb](https://www.themoviedb.org/).  \n",
    "\n",
    "The easy way to input this data is to download the zip file onto our computer, unzip it, and read the individual csv files using `read.csv()`.  But **anyone can do it the easy way**.  We want to automate this, so we can redo it without any manual steps.  This takes some effort, but once we have it down we can apply it to lots of other data sources.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate file download \n",
    "\n",
    "We're looking for an automated way, so that if we do this again, possibly with updated data, the whole process is in our code.  Automated data entry involves these steps: \n",
    "\n",
    "* Get the file.  We use the [requests](http://docs.python-requests.org/) package, which handles internet files and comes pre-installed with Anaconda. This kind of thing was hidden behind the scenes in the Pandas `read_csv` function, but here we need to do it for ourselves. The package authors add:  \n",
    ">Recreational use of other HTTP libraries may result in dangerous side-effects, including: security vulnerabilities, verbose code, reinventing the wheel, constantly reading documentation, depression, headaches, or even death.\n",
    "* Convert to zip.   Requests simply loads whatever's at the given url. The [io](https://docs.python.org/3.5/library/io.html) module's `io.Bytes` reconstructs it as a file, here a zip file.  \n",
    "* Unzip the file.  We use the [zipfile](https://docs.python.org/3.5/library/zipfile.html) module, which is part of core Python, to extract the files inside.   \n",
    "* Read in the csv's.  Now that we've extracted the csv files, we use `read_csv` as usual.  \n",
    "\n",
    "We found this [Stack Overflow exchange](http://stackoverflow.com/questions/23419322/download-a-zip-file-and-extract-it-in-memory-using-python3) helpful. \n",
    "\n",
    "**Digression.**  This is probably more than you want to know, but it's a reminder of what goes on behind the scenes when we apply `read_csv` to a url.  Here we grab whatever is at the url.  Then we get its contents, convert it to bytes, identify it as a zip file, and read its components using `read_csv`.  It's a lot easier when this happens automatically, but a reminder what's involved if we ever have to look into the details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get \"response\" from url \n",
    "url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "r = requests.get(url) \n",
    "\n",
    "# describe response \n",
    "print('Response status code:', r.status_code)\n",
    "# Here is a link to what different coes mean: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n",
    "# Quick summary 200 is good, 400 is bad, i.e. its not responsive may not exist. Othere indicate something\n",
    "# more complicated might be going on.\n",
    "\n",
    "print('Response type:', type(r))\n",
    "print('Response .content:', type(r.content)) \n",
    "print('Response headers:\\n', r.headers, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bytes to zip file  \n",
    "mlz = zf.ZipFile(io.BytesIO(r.content)) \n",
    "print('Type of zipfile object:', type(mlz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's in the zip file?\n",
    "# This is really cool...\n",
    "mlz.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and read csv's\n",
    "movies  = pd.read_csv(mlz.open(mlz.namelist()[2]))\n",
    "ratings = pd.read_csv(mlz.open(mlz.namelist()[3]))\n",
    "\n",
    "tags = pd.read_csv(mlz.open(mlz.namelist()[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whip through the dataframes and look what is going on...\n",
    "\n",
    "for df in [movies, ratings]:\n",
    "    print('Type:', type(df))\n",
    "    print('Dimensions:', df.shape)\n",
    "    print('Variables:', list(df))\n",
    "    print('First few rows', df.head(3), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging ratings and movie titles \n",
    "\n",
    "The movie ratings in the dataframe `ratings` give us individual opinions about movies, but they don't include the name of the movie.   Why not?  Rather than include the name every time a movie is rated, the MovieLens data associates each rating with a movie code, than stores the names of movies associated with each movie code in the dataframe `movies`.  We run across this a lot:  some information is in one data table, other information is in another. Here, since this was an intientional decision to split up the files, there is a natural ``key`` that can be used to link the two. \n",
    "\n",
    "Our **want** is therefore to add the movie name to the `ratings` dataframe.  We say we **merge** the two dataferames.  There are lots of ways to merge.  Here we do one as an illustration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "Here's roughly what's involved in what we're doing.  We take the `movieId` variable from `ratings` and look it up in `movies`.  When we find it, we look up the `title` and add it as a column in `ratings`.  The variable `movieId` is common, so we can use it to link the two dataframes.  **Is this a many-to-one merge or one-to-one merge?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions of ratings:', ratings.shape)\n",
    "print('Dimensions of movies:', movies.shape)\n",
    "\n",
    "# Again note how there are more ratings than movies...\n",
    "# It is a many-to-one merge, assign the many ratings\n",
    "# to a ONE movie title. \n",
    "\n",
    "combo = pd.merge(ratings, movies,   # left and right df's\n",
    "                 how='left',        # add to left \n",
    "                 on='movieId',      # link with this variable/column \n",
    "                 indicator=True) \n",
    "\n",
    "print('Dimensions of new df:', combo.shape)\n",
    "\n",
    "combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "- Merge the tags to our new DataFrame. Which movie is the \"sandra 'boring' bullock\" one?\n",
    "\n",
    "- Now try the original merge (ratings and movies) with right part...What is going on here?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combo = pd.merge(ratings, movies,   # left and right df's\n",
    "                 how='right',        # add to left \n",
    "                 on='movieId',       # link with this variable/column \n",
    "                 indicator=True) \n",
    "\n",
    "print('Dimensions of new df:', combo.shape)\n",
    "\n",
    "combo.head()\n",
    "\n",
    "combo[combo[\"_merge\"] == \"right_only\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Practice with grouping, slicing, visualization**\n",
    "\n",
    "Some of these we know how to do, the others we don't.  For the ones we know, what is the answer?  For the others, what (in loose terms) do we need to be able to do to come up with an answer?  \n",
    "\n",
    "* What is the overall average rating?  By genre?\n",
    "* What is the overall distribution of ratings?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Rating\",round(combo.rating.mean(),2))\n",
    "print(\"Median Rating\",round(combo.rating.median(),2))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "combo.rating.plot(ax = ax, kind = \"hist\", bins = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets ask a couple of more questions, this practices `groupby`\n",
    "* What is the average rating of each movie?  \n",
    "* How many ratings does each movie get? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = combo.groupby(\"title\")\n",
    "# This is similar syntax to that used in the Chiplotle data\n",
    "# Create the groupby object.\n",
    "\n",
    "movie_ratings = grouped.rating.agg([\"count\",\"mean\"])\n",
    "# Then aggregate the ratings...how? In two ways, we pass a list\n",
    "# saying, take the count, take the mean.\n",
    "\n",
    "movie_ratings.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets look at movies, with only at least 20 ratings, and then sort them.\n",
    "# So what are the best rated movies...\n",
    "\n",
    "movie_ratings[movie_ratings[\"count\"] >= 20.0][\"mean\"].sort_values().tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the final thing we could do is create a scatter plot of average rating versus count, i.e. do better rated movies have more ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(movie_ratings[movie_ratings[\"count\"] >= 50.0][\"mean\"], \n",
    "           movie_ratings[movie_ratings[\"count\"] >= 50.0][\"count\"], alpha = 0.50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Rather than grouping by movie, you could group by the rater... are there some raters that are systematically negative, systematically positive? Even more interesting question, note that there is a time dimension, so maybe there is a time effect or life cycle component? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
